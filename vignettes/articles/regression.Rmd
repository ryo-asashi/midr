---
title: "Interpretation of Regression Models"
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

pkgs <- c("mlbench", "gridExtra", "ggplot2", "Metrics", 
          "glmnet", "ranger", "e1071", "nnet", "rpart", "gam", "xgboost")
for (pkg in pkgs) {
  if (!(pkg %in% installed.packages())) {
    install.packages(pkg)
  }
}
```

```{r setup, message = FALSE}
# load required packages
library(midr)
library(ggplot2)
library(gridExtra)
library(Metrics)
theme_set(theme_midr())
```

## Regression Task

Here, we use a benchmark task, originally described in Friedman (1991) and Breiman (1996), and provided by the `mlbench` package. The data have 10 independent predictor variables $x_1,x_2,...,x_{10}$ each uniformly distributed on the interval $[0,1]$, and the response variable $y$ , generated according to the following formula with disturbance term $\epsilon\ {\sim}\ \mathcal{N}{(0, 1)}$.

$$
y=10\sin{(\pi{x_1}{x_2})+20{(x_3-0.5)}^2+10x_4+5x_5+\epsilon}
$$

```{r mlbench}
# benchmark dataset for regression task
library(mlbench)
set.seed(42)
train  <- as.data.frame(mlbench.friedman1(n = 500L))
test   <- as.data.frame(mlbench.friedman1(n = 500L))
mtrain <- as.data.frame(mlbench.friedman1(n = 2500L))[, -11L]
```

The following plots show the effect of each predictor variable on the response. For $x_1$ and $x_2$ , the interaction effect is shown by the colored lines: the effect of $x_1$ depends on the value of $x_2$ (blue for 0 and red for 1) and *vice versa*.

```{r dataviz, echo = FALSE}
# visualization of the component functions
f1 <- function(y) {function(x) 10 * sin(pi * x * y)}
f3 <- function(x) 20 * (x - 0.5)^2
f4 <- function(x) 10 * x
f5 <- function(x) 5 * x
cols <- color.theme("Zissou 1")$ramp
pl <- ggplot() + xlim(0, 1) + xlab("x1, x2") +
  geom_function(fun = f1(0.0), col = cols(0.0)) +
  geom_function(fun = f1(0.1), col = cols(0.1)) +
  geom_function(fun = f1(0.2), col = cols(0.2)) +
  geom_function(fun = f1(0.3), col = cols(0.3)) +
  geom_function(fun = f1(0.4), col = cols(0.4)) +
  geom_function(fun = f1(0.5), col = cols(0.5)) +
  geom_function(fun = f1(0.6), col = cols(0.6)) +
  geom_function(fun = f1(0.7), col = cols(0.7)) +
  geom_function(fun = f1(0.8), col = cols(0.8)) +
  geom_function(fun = f1(0.9), col = cols(0.9)) +
  geom_function(fun = f1(0.05), col = cols(0.05)) +
  geom_function(fun = f1(0.15), col = cols(0.15)) +
  geom_function(fun = f1(0.25), col = cols(0.25)) +
  geom_function(fun = f1(0.35), col = cols(0.35)) +
  geom_function(fun = f1(0.45), col = cols(0.45)) +
  geom_function(fun = f1(0.55), col = cols(0.55)) +
  geom_function(fun = f1(0.65), col = cols(0.65)) +
  geom_function(fun = f1(0.75), col = cols(0.75)) +
  geom_function(fun = f1(0.85), col = cols(0.85)) +
  geom_function(fun = f1(0.95), col = cols(0.95)) +
  geom_function(fun = f1(1.0), col = cols(1.0))
grid.arrange(nrow = 2,
  pl,
  ggplot() + xlim(0, 1) + xlab("x3") + geom_function(fun = f3),
  ggplot() + xlim(0, 1) + xlab("x4") + geom_function(fun = f4),
  ggplot() + xlim(0, 1) + xlab("x5") + geom_function(fun = f5)  
)
```

For each model, we fit a regression model using the `train` data of 500 observations and an interpretative MID surrogate of the target model using the `mtrain` data of 2500 observations without the response variable. We then evaluate the predictive accuracy of the target model and the interpretative accuracy of the MID surrogate based on the RMSE between the `test` and model prediction or the two predictions, respectively.

```{r utils}
# define utility functions for the following chunks
effect_plots <- function(object) {
  plots <- mid.plots(mid, terms = paste("x", 1:5, sep = "."))
  plots[[6]] <- ggmid(mid, "x.1:x.2", main.effects = TRUE) +
    theme(legend.position = "none")
  plots
}
ice_theme <- color.theme("Zissou 1", type = "sequential")
ice_plots <- function(object, data) {
  p1 <- ggmid(mid.conditional(mid, "x.1", data = data),
              var.color = x.2, type = "centered", theme = ice_theme) +
    theme(legend.position = "bottom")
  p2 <- ggmid(mid.conditional(mid, "x.2", data = data),
              var.color = x.1, type = "centered", theme = ice_theme) +
    theme(legend.position = "bottom")
  list(p1, p2)
}
importance_plot <- function(object) {
  ggmid(mid.importance(object), "dotchart", theme = "Set 1") +
    theme(legend.position = "bottom")
}
evaluation_plot <- function(pred, pred_mid, actual) {
  rmse_vs_test <- rmse(pred, actual)
  rmse_vs_mid <-  rmse(pred, pred_mid)
  ggplot() + scale_color_theme("Accent") +
    geom_point(aes(x = preds, y = test$y, col = "vs test")) +
    geom_point(aes(x = preds, y = preds_mid, col = "vs mid")) +
    geom_abline(slope = 1, intercept = 0, col = "black", lty = 2) +
    theme(legend.position = "bottom") +
    labs(x = "model-prediction", y = "mid-prediction / test") +
    annotate(
      "text", family = "serif", size = 3,
      x = min(preds) + diff(range(preds)) / 10,
      y = max(actual) - diff(range(actual) / 10),
      label = sprintf("RMSE\nvs test: %.3f\nvs mid: %.3f",
                      rmse_vs_test, rmse_vs_mid)
    )
}
```

## Additive Models

### Linear Model

```{r stats_lm}
model <- lm(y ~ ., train)
mid <- interpret(y ~ .^2, mtrain, model)
print(mid)
grid.arrange(grobs = effect_plots(mid), nrow = 2L)
preds <- predict(model, test)
preds_mid <- predict(mid, test)
grid.arrange(nrow = 1L, importance_plot(mid),
             evaluation_plot(preds, preds_mid, test$y))
# grid.arrange(grobs = ice_plots(mid, test[1:100, ]), nrow = 1L)
```

### Regularized GLM

```{r glmnet_glmnet, message=FALSE}
library(glmnet)
model <- glmnet(x = as.matrix(train[, -11]), y = train[, 11])
# prediction with arbitrarily chosen lambda
pred_rglm <- function(object, newdata)
  predict(object, newx = as.matrix(newdata), s = object$lambda[9])
mid <- interpret(y ~ .^2, train[, -11], model, pred.fun = pred_rglm)
preds <- predict(model, as.matrix(test[, -11]), s = model$lambda[9])
preds_mid <- predict(mid, test)
print(mid)
grid.arrange(grobs = effect_plots(mid), nrow = 2L)
grid.arrange(nrow = 1L, importance_plot(mid),
             evaluation_plot(preds, preds_mid, test$y))
# grid.arrange(grobs = ice_plots(mid, test[1:100, ]), nrow = 1L)
```

### Generalized Additive Model

```{r gam_gam, message=FALSE}
library(gam)
model <- gam(y ~ s(x.1) + s(x.2) + s(x.3) + s(x.4) + s(x.5) +
             s(x.6) + s(x.7) + s(x.8) + s(x.9) + s(x.10),
             family = gaussian, data = train)
mid <- interpret(y ~ .^2, train, model)
preds <- predict(model, test)
preds_mid <- predict(mid, test)
print(mid)
grid.arrange(grobs = effect_plots(mid), nrow = 2L)
grid.arrange(nrow = 1L, importance_plot(mid),
             evaluation_plot(preds, preds_mid, test$y))
grid.arrange(grobs = ice_plots(mid, test[1:100, ]), nrow = 1L)
```

## Neural Network

### Single Hidden Layer Network

```{r nnet_nnet}
library(nnet)
set.seed(42)
model <- nnet(y ~ ., train, size = 5, linout = TRUE, maxit = 1e3, trace = FALSE)
mid <- interpret(y ~ .^2, mtrain, model)
preds <- predict(model, test)
preds_mid <- predict(mid, test)
print(mid)
grid.arrange(grobs = effect_plots(mid), nrow = 2L)
grid.arrange(nrow = 1L, importance_plot(mid),
             evaluation_plot(preds, preds_mid, test$y))
grid.arrange(grobs = ice_plots(mid, test[1:100, ]), nrow = 1L)
```

## Support Vector Machine

### RBF Kernel SVM

```{r e1071_svm}
library(e1071)
model <- svm(y ~ ., train, kernel = "radial")
mid <- interpret(y ~ .^2, mtrain, model)
preds <- predict(model, test)
preds_mid <- predict(mid, test)
print(mid)
grid.arrange(grobs = effect_plots(mid), nrow = 2L)
grid.arrange(nrow = 1L, importance_plot(mid),
             evaluation_plot(preds, preds_mid, test$y))
grid.arrange(grobs = ice_plots(mid, test[1:100, ]), nrow = 1L)
```

## Tree Based Models

### Gradient Boosting Trees

```{r xgboost_xgboost}
library(xgboost)
params <- list(eta = .1, subsample = .7, max_depth = 5)
set.seed(42)
model <- xgboost(as.matrix(train[, -11]), train[, 11], nrounds = 100,
                 params = params, verbose = 0)
mid <- interpret(y ~ .^2, as.matrix(mtrain), model)
preds <- predict(model, as.matrix(test[, -11]))
preds_mid <- predict(mid, test)
print(mid)
grid.arrange(grobs = effect_plots(mid), nrow = 2L)
grid.arrange(nrow = 1L, importance_plot(mid),
             evaluation_plot(preds, preds_mid, test$y))
grid.arrange(grobs = ice_plots(mid, test[1:100, ]), nrow = 1L)
```

### Random Forest

```{r ranger_ranger}
library(ranger)
set.seed(42)
model <- ranger(y ~ ., train, mtry = 5)
mid <- interpret(y ~ .^2, mtrain, model)
preds <- predict(model, test)$predictions
preds_mid <- predict(mid, test)
print(mid)
grid.arrange(grobs = effect_plots(mid), nrow = 2L)
grid.arrange(nrow = 1L, importance_plot(mid),
             evaluation_plot(preds, preds_mid, test$y))
grid.arrange(grobs = ice_plots(mid, test[1:100, ]), nrow = 1L)
```

### Decision Tree

```{r rpart_rpart}
library(rpart)
model <- rpart(y ~ ., train)
mid <- interpret(y ~ .^2, mtrain, model)
preds <- predict(model, test)
preds_mid <- predict(mid, test)
print(mid)
grid.arrange(grobs = effect_plots(mid), nrow = 2L)
grid.arrange(nrow = 1L, importance_plot(mid),
             evaluation_plot(preds, preds_mid, test$y))
grid.arrange(grobs = ice_plots(mid, test[1:100, ]), nrow = 1L)
```
